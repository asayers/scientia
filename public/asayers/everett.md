% The Everett Interpretation
% Alex Sayers


In 1957 Hugh Everett proposed a new model for interpreting quantum mechanics. The purpose of this interpretation was to circumvent the Measurement Problem, and the proposed method was to simply rid oneself of the components of the theory which give rise to it: namely, collapse (and with it the Born Rule). Everett left academia early in his career, and so his proposal was left to be itself interpreted by others. Most take it to be a \emph{many worlds} theory, but such theories seem quite dissimilar to Everett's initial proposal. There are many alternative readings, each garnering some small following; most are engineered to avoid the fundamental problem of the many worlds approach---the establishment of a preferred basis. Among them are the many minds and many histories interpretations, but I will not talk about them. Rather I will return to a purer form of the theory---one which I believe is closer to Everett's original intent.

So what did Everett actually propose? In short, we are to take the unitary dynamics as describing the evolution of all wavefunctions, and all systems to be describable by such a wavefunction. Clearly we have rid ourselves of the Measurement Problem: there is no longer a distinction between microsystems and macrosystems---between subsystem and apparatus---since both are simply quantum systems which become entangled through some appropriate measurement interaction. There is now no problem in speaking of the universal quantum system: it is simply an all-describing wavefunction in some incredibly high-dimensions Hilbert space, which evolves unitarily according to the TDSE.

This is all well and good, but surely we have sacrificed empirical adequacy? After all, if there is no collapse, there are no determinate values; and whenever we measure the value of an observable, isn't it determinate? Everett explains that in his model, the measurement interactions are such that the brains of observers become entangled with the system under observation, and end up in a superposition of states. In each of these brain-states, a memory is held of obtaining a different value upon measurement. Furthermore, the interaction is such that a correlation is established between the superposed states of the system and the resulting states of the brain.

Arguably, we now have enough for a workable model---certainly, this was the belief of Everett, and Albert and Loewer besides. Many have felt that justice still hasn't been done to the phenomena, and as such the theory is empirically inadequate. To the end of completing the theory, the notion of a branch is introduced. Say the combined subsystem-observer system begins in the state $\frac{1}{\sqrt{2}}(\ket{\uparrow}+\ket{\downarrow})\kronecker\ket{``Ready"}$, and that after the interaction measurement has occurred it is in the state $\frac{1}{\sqrt{2}}(\ket{\uparrow}\kronecker\ket{``Spin~up"}+\ket{\downarrow}\kronecker\ket{``Spin~down"})$. According to the many worlds theorist, the interaction has caused the world of the initial state to branch into two separate worlds: one in which the observer measures spin up, and the other in which he measures spin down. This example illustrates the crucial feature which any candidate branching dynamics must possess: for every term in the universal wavefunction which corresponds to a particular measurement record, a branching must have occurred to provide that term with its own world.

This clearly removes any concerns one might have regarding determinate values: it is constructed to make sure that in every world, all measurement records are indeed determinate. We might ask ourselves: what distinguishes something which must have a determinate value from something which needn't? The distinction between the microscopic and macroscopic seems to have found its way into the many worlds theory, and the introduction of the branching dynamics to account for their difference seems as \emph{ad hoc} as the collapse dynamics of the orthodox model.

Worse still, the requirements placed on any candidate branching dynamics forces it to establish a preferred basis: the terms of the superposition describing a piece of apparatus must be given their own worlds, but only when that superposition is expanded in a particular basis. This is because we cannot tolerate indeterminacy about what worlds there are as we can indeterminacy about the form of the universal wavefunction. An ontology with certain worlds cannot be said to be equivalent to another which comprises the worlds of a different expansion. Thus we must select a basis as being the one to govern what worlds are to be formed when branching occurs.

Some proposals have been made. We could, for instance, take the position eigenbasis as privileged, as de Broglie and Bohm do. Alternatively, we could allow decoherence to do the work for us, as David Wallace does. Unfortunately, these accounts yet suffer from the preferred basis problem. There also remains the question of when the branching occurs. Of course, this depends on the specifics of the branching dynamics in question, but it seems unlikely that a suitable dynamics exists which is also Lorentz covariant.

With so many problems being introduced by the many worlds modification, we might ask ourselves whether we need it. The many worlds theory is essentially a hidden variable theory: the universal wavefunction evolves unitarily, while the branching dynamics keep the necessary values determinate. The hidden variable records which branch we are actually experiencing. While it is somewhat difficult to tell, it doesn't seem as though Everett had such a theory in mind, and it seems to me that the many worlds interpretation is in fact a betrayal of Everett's radical idea. The only entity to which we should be ontologically committed is the universal wavefunction. There is no need to select a basis, and there is no need for a branching dynamics which will conflict with relativity. Many worlds is sugar on top of the basic theory, which sweetens it for the realist: it provides an easier way to interpret the theory ontologically, but it is unnecessary, and even problematic.

If we return to a form of the theory without any such augmentation, we arrive at a view on which the universe has a single history, and a single actualised entity. There is no privileged branch, and so there are no hidden variables. There is no branching dynamics, and so no need for a preferred basis. It is deterministic, and the role of causation is clear (interaction via the hamiltonian). There are no \emph{ad hoc} attempts to restore determinacy to values one feels really ought to be determinate. But how do we reconcile this with the apparent determinacy of the observable world? Isn't this what drove us posit the additional worlds in the first place? We do it through the introduction of something I shall call a \emph{branch-indexed semantics}.

Consider the philosophy of time.\footnote{Simon Saunders expresses similar considerations.} It might be argued that the intuitive position is that of the Presentist, but this has been shown to be at odds with Special Relativity. Thus we conclude that the entire history of the universe really exists, as concretely as do those things seen at the present. To account for the truth of the statement, ``Elizabeth is the Queen,'' we must introduce a \emph{tensed semantics}. Thus the truth of any statement depends on the time at which it is made.

There can be no truths which are not time-indexed in this way, and so it makes no sense to enquire after the current position of the universe on the timeline. The answer depends, of course, on when the question is asked; there is no absolute, timeline-independent answer. This conclusion is striking, and at first seems not to account for the phenomenology of passage through time. Yet it has become accepted into orthodoxy,\footnote{It strikes me that I may have just presented a highly unorthodox philosophy of time. It is, at any rate, the one to which I am committed.} since the alternatives imply testable falsehoods.

So in order to avail ourselves properly of the collapse dynamics of QM, we must posit the real existence of the universal wavefunction, and acquaint ourselves with a branch-indexed semantics. As was the case in the philosophy of time, so in the philosophy of QM we have abandoned the ontology which comprises only those things which make up our experience and embraced one which includes all possible experiences (given the initial conditions).

Turning to the new semantics, a statement is true if and only if it corresponds to facts regarding the term of the universal superposition in which its utterance is a factor. Hence: $\frac{1}{\sqrt{2}}(\ket{\uparrow}\kronecker\ket{``Spin~up"}+\ket{\downarrow}\kronecker\ket{``Spin~down"})$---both claims are true, because they are evaluated relative to the facts about the term in which they appear. Just as the feeling that there is an absolute, external fact about the present is mistaken, so proponents of the view I describe must admit that the apparent determinacy of macroscopic observations is illusory. Nothing is determinate save the universal wavefunction itself; all appearances to the contrary stem from the branch-indexed semantics I have described.

One problem remains, and it is a problem for all Everettian approaches: how do we incorporate probability. It is these probabilities which won QM recognition enough to make its foundational problems troubling; we cannot solve them by sacrificing its predictive power. Under the view I have espoused above, the universal superposition clearly contains many terms in any given expansion, each of which carries a coefficient. These are predictively unimportant. To generate a prediction we must first determine which term in the superposition is being experienced (the universal wavefunction having been expanded in an appropriate basis), and then we must calculate its unitary evolution. Given that an experiment is presumably in our immediate future, this evolution will involve entanglement with some other system; as such, the term we have identified will become a superposition. The mod square of the coefficients, when divided by the mod square of the current state's coefficient---give the conditional probability---the degree to which we might expect to experience each brain-state.

This may seem just as \emph{ad hoc} as the Born Rule, but this is tolerable since the addition to the theory regards the extraction of predictions, but doesn't change the state in any way. Besides this, it gives us a clear way of interpreting probabilistic language: when someone says that an outcome is likely, they aren't referring to dispositions or frequencies---they are making a claim about the future of the universal wavefunction, and the coefficients of its various terms. It is arguable that probabilistic claims are really claims about possible worlds; in this case, Everett simply exposes this fact in a precice way.

Everett, then, provided a radical new way of thinking of QM. He sought to avoid the Measurement Problem by simply elliminating the troublesome collapse dynamics, and showing how correlations between systems and brains account for experience. The intuition that macroscopic values are determinate is strong, however, and many have been drawn to take a many worlds view. Such accounts share difficulties with the orthodox interpretation, however, and even add some uniquely their own. If one is to be an Everettian, one should commit to Everett's big idea: that there is nothing but the universal wavefunction, evolving unitarily according to some large all-describing hamiltonian. All statements must be taken as being implicitly branch-indexed, all facts state-relative. Some difficulty remains incorporating probability into our picture and reproducing the legendary predictions of the QM formalism, but the prospects that such an incorporation might be produced seem reasonably good. If a stripped-down, relative-facts kind of Everett interpretation could be shown to produce the standard QM predictions, then it would be a very compelling interpretation indeed.
